---
title: "HW4-Migration Solution"
author: "Yayu Zhou"
date: "3/15/2020"
output:
  html_document:
    toc: yes
    toc_float: yes
fontsize: 12pt
header-includes: \usepackage{amsmath}
---

```{r include=FALSE}
# load packages
library(data.table)
library(dplyr)
library(car)
library(glmnet)
library(expm)
library(knitr)
library(rmarkdown)

opts_chunk$set(message = F, warning = F, echo = T,
               fig.width = 8, fig.height = 5, fig.align = "center")
knit_hooks$set(inline = function(x){
  prettyNum(x, big.mark=",")
  print(round(x, 4))
})
```

## Problem 1
```{r, include = F}
np = fread("~/Documents/Google Drive/Northwestern University/Winter 2020/MSIA 421/Homework/np.csv", sep = " ", na.strings = ".")
```

### a)
```{r}
np = np %>%
  group_by(SubscriptionId) %>%
  mutate(nextchurn = lead(churn), nextprice = lead(currprice)) %>%
  ungroup()
```

### b)
```{r}
np = np %>% mutate(t = factor(t))
table(np$t)
```

### c)
```{r}
fit1 = glm(nextchurn ~ t+trial+nextprice+regularity+intensity, family = binomial, data = np)
fit2 = glm(nextchurn ~ t+trial+nextprice+regularity, family = binomial, data = np)
fit3 = glm(nextchurn ~ t+trial+nextprice+intensity, family = binomial, data = np)
summary(fit1) 
summary(fit2) 
summary(fit3)

round(cor(na.omit(np[, c("nextchurn", "trial", "nextprice", "regularity", "intensity")])), 4)
vif(fit1); vif(fit2); vif(fit3);
```

1. In the first model,

   * trial period drives churn: if in a trial period, more likely to churn
   * current price drives churn: higher the price, more likely to churn
   * regularity drives retention: the greater regularity, the less likely to churn
   * intensity has insignificant effect on churn
   
2. In the second model, without controling for intensity, same conclusions for other predictors

3. In the third model, without controling for regularity

   * same conclusions for other predictors
   * intensity drives retention (it has negative and significant effect): the more intensity, the less likely to churn
   
4. Looking at the correlation matrix, we see that regularity and intensityare moderately positively correlated. The result from VIFs also confirms that. This is why intensity is not statistically significant when regularity isincluded in the model, but is when it is alone.

5. Trial appears to be positively associated with churn. It is not clear if the trial itself is the cause or if customers who are offered trials are more likely to churn anyway.


### d)
```{r}
fit4 = glm(nextchurn~t+trial+nextprice+sports1+news1+crime1+life1+obits1+business1+opinion1, family = binomial, data = np)
summary(fit4)

fit4.2 = glm(nextchurn~t+trial+nextprice+regularity+sports1+news1+crime1+life1+obits1+business1+opinion1, family = binomial, data = np)
summary(fit4.2)
round(cor(na.omit(np %>% select(nextchurn, regularity, sports1:opinion1))), 4)
```

Without controling regularity, sports and news drive retention. This conclusion changes if we include regularity in the model. Note that regularity is moderately correlated with all the content variables, which explains that after controling for regularity, none of the effects are significant. 


### e)
```{r}
setnames(np, old = c("Loc2", "Loc3", "Loc4"), new = c("loc2", "loc3", "loc4"))
fit5 = glm(nextchurn~t+trial+nextprice+loc1+loc2+loc3+loc4, family = binomial, data = np)
summary(fit5)
fit6 = glm(nextchurn~t+trial+nextprice+regularity+loc1+loc2+loc3+loc4, family = binomial, data = np)
summary(fit6)
```

Without controling regularity, loc1 is negative and significant, which drives retention. After controling regularity, none of the location variable is significant.


### f)
```{r}
fit7 = glm(nextchurn~., family = binomial, data = np %>% select(nextchurn, t, trial, nextprice, SrcGoogle:SrcGoogleAd))
summary(fit7)
```
ScrGoogle and ScrGoogleNews are border-line negatively associated with churn.


### g)
```{r}
fit8 = glm(nextchurn~t+trial+nextprice+mobile+tablet+desktop, family = binomial, data = np)
summary(fit8)
```

Both tablet (border-line) and desktop drives retention.


### h)
```{r}
fit9 = glm(nextchurn~., family = binomial, data = np %>% select(-c(SubscriptionId, churn, currprice)))
summary(fit9)
```

When including all variables in the model, only trail, nextprice and regularity are significant. The conclusionis consistent with the previous part.

Lasso result is as following:
```{r}
x.model = model.matrix(nextchurn ~., data = np %>% select(-c(SubscriptionId, churn, currprice)))
set.seed(421)
fit10 = cv.glmnet(x = x.model, y = na.omit(np)$nextchurn, alpha = 1, family = "binomial", standardize = T, nfold = 10)
## trace plot
plot(fit10, xvar = "lambda", main = "Lasso", type = "l", ylab = "Coeff")
lambda = fit10$lambda.min
lambda
coeff = coef(fit10$glmnet.fit)[, which(fit10$lambda == lambda)]
coeff 
```

From lasso result, regularity, intensity, desktop and SrcNewsletter drive retention, while trial, nextprice and SrcGoogleAd drive churn.


### i)
 
 * No association with churn: crime1, life1, obits1, business1, opinion1, mobile, loc2, loc3, loc4, SrcDirect, SrcElm, SrcSocial, SrcBingYahooAol, SrcLegacy
 * Strong drivers of churn: trial, nextprice
 * Strong drivers of retention: regularity, intensity, desktop
 * Questionable drivers of churn: SrcGoogleAd
 * Questionable drivers of retention: tablet, SrcNewsletter
 
\
\


## Problem 2
$$n_t' = n_{t-1}'P + a' = (n_{t-2}P + a')P + a' = n_{t-2}P^2 + a'(P+I) = ... = n_0'P^t + a'(P^{t-1} + ...+P^2+P+I)$$
From class we know
$$I + P + P^2 + ... +P^{t-1} = (I-P^t)(I-P)^{-1}$$
Hence
$$n_t' = n_0'P^t + a'(I-P^t)(I-P)^{-1}$$

\
\


## Problem 3
```{r, include = F}
npmarkov = read.csv("~/Documents/Google Drive/Northwestern University/Winter 2020/MSIA 421/Homework/npmarkov.csv")
colnames(npmarkov) = c(names(npmarkov)[-1], "PV")
```

### a)
```{r}
P = prop.table(x = as.matrix(npmarkov[, -ncol(npmarkov)]), margin = 1)
P
```


### b)
```{r}
r = c(0, 0, 1, 1, 10, 10, 0, 0)
v = 0.002*npmarkov$PV + r
v
```

\
\


For all the following parts, we are using the following function to do the calculation
```{r}
# a function to compute the expected profit, CLV and CE at time t; also, the long term CLV and CE
CLVcalculatorSF = function(t = Inf, P, v, n0, d, a = rep(0, times = nrow(P))){
  ## k = k step transition
  ## data = model data, each row is a subscriber with state in period t
  ## P = transition matrix
  ## v = vector value
  ## n = initial # of subs
  ## d = discount rate
  ## a = # of acquired people
  I = diag(nrow(P))
  
  # compute expected profit, CLV and CE
  if(t == Inf){   ## if in infinity
    CLV = solve(I-P/(1+d)) %*% v  ## CLV
    CE = t(n0) %*% solve(I-P/(1+d)) %*% v  ## CE
    result = rbind(n0, t(CLV))
    rownames(result) = c("n0", "CLV")
    return(list(summary = result, CE = as.numeric(CE)))
  }else{  ## in finite time
    nt = t(n0)
    countTable = matrix(nrow = t, ncol = ncol(P)+1)
    CE = c()
    for(i in 1:t){  ## better to do it recursively
      # nt = t(n0) %*% (P%^%i) + t(a) %*% (I - (P%^%i)) %*% solve(I - P, tol = 1e-20)  ## might not be correct as conditional number is too small
      nt = nt %*% P + t(a)
      countTable[i, ] = c(t = i, nt)
      CE[i] = nt %*% v/((1+d)^i)
    }
    
    colnames(countTable) = c("t", colnames(P))
    CE = as.numeric(sum(CE) + t(n0) %*% v)
    result = data.frame(T = t, ProsSubs = sum(countTable[t, 2:3]), TrialSubs = sum(countTable[t, 4:5]), FullSubs = sum(countTable[t, 6:7]), ChurnSubs = sum(countTable[t, 8:9])) %>%
      mutate(totalSubs = TrialSubs + FullSubs,
             CE = CE)
    CLV = (I-(P/(1+d))%^%t) %*% solve(I-P/(1+d)) %*% v ## CLV(T)
    return(list(summary = result, migrationTable = countTable, CLV = as.numeric(CLV)))
  }
}
```


### c)
```{r}
d = 0.01
n0 = c(5000, 5000, 1000, 1000, 3000, 3000, 4000, 4000)  ## initial n
CE1 = CLVcalculatorSF(t = Inf, P = P, v = v, n0 = n0, d = d)$CE
CE1
```

As t goes to infinity, we find CE = `r CE1`.


### d)
```{r}
v2 = 0.001*npmarkov$PV + r
CE2 = CLVcalculatorSF(t = Inf, P = P, v = v2, n0 = n0, d = d)$CE
CE2  ## 10730531
CE2-CE1  ## -171991.8; decreases
```

If we cut the number of advertisements in half, so that the ad revenue is $0.001/PV, then we get CE = `r CE2`, which decreases `r abs(CE2-CE1)` compared to part c).


### e)
```{r}
CE3 = CLVcalculatorSF(t = 36, P = P, v = v2, n0 = n0, d = d)$summary$CE
CE3  ## 3810786
```
We finite horizon $t = 36$, we compute $CE = `r CE3`$. 

Note that the answer $CE = `r CE3 - t(n0)%*%v2`$ is also considered to be right, in this case, students do not add the term for $t = 0$, i.e., $CE(0) = n_0' v$


### f)
```{r}
a = c(100, 100, 0, 0, 0, 0, 0, 0)
temp = CLVcalculatorSF(t = 36, P = P, v = v2, n0 = n0, d = d, a = a)
CE4 = temp$summary$CE
CE4  ## 4382980;
totalsubs4 = temp$summary$totalSubs
floor(totalsubs4)  ## 14708
CE4-CE3 ## 572193.9
```
Suppose that you acquire 200 new prospects each month, 100 with low regularity and 100 with high regularity. Now we compute $CE = `r CE4`$, the total number of subscribers is `r floor(totalsubs4)`. Compared to part e), this CE increases `r CE4-CE3`.

Note that the answer $CE = `r CE4 - t(n0)%*%v2`$ is also considered to be right.

\
\


### g)
```{r}
PP = P
for(i in 1:2){
  for(j in 1:2)
    PP[i, j] = P[i, j]-0.01
}
for(i in 3:6){
  for(j in 7:8)
    PP[i, j] = P[i, j]-0.01
}
for(i in 1:2){
  for(j in 3:4)
    PP[i, j] = P[i, j]+0.01
}
for(i in 3:6){
  for(j in 5:6)
    PP[i, j] = P[i, j]+0.01
}


temp = CLVcalculatorSF(t = 36, P = PP, v = v2, n0 = n0, d = d, a = a)
CE5 = temp$summary$CE
CE5  ## 5369459
totalsubs5 = temp$summary$totalSubs
floor(totalsubs5)  ## 19949
CE5-CE4 ## 986479.1
```

Here we compute $CE = `r CE5`$, the total number of subscribers is `r floor(totalsubs5)`. Compared to part f), this CE increases `r CE5-CE4`.

Note that the answer $CE = `r CE5 - t(n0)%*%v2`$ is also considered to be right.


















