---
title: "Migration HW - Kristiyan Dimitrov"
author: "Kristiyan Dimitrov"
date: "2/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

```{r}
library(tidyverse)
library(car)
library(glmnet)
np = read.csv('np2.csv', na.strings=".", stringsAsFactors=FALSE)
head(np,100)
```

## a) - Creating nextchurn & nextprice

The below code creates a column called nextchurn, which is just the churn column shifted one cell up for each customer.
The empty cell left over where a new customer starts is left as an NA. In other words, in the latest month of the customer's life we don't know if he/she will churn next month. The important thing is that THIS month we have an indicator if he/she will churn next month.

```{r}
np = np %>% 
  group_by(SubscriptionId) %>% 
  mutate(nextchurn = lead(churn, order_by = t)) # This creates a column called nextchurn, which is just the churn column shifted one cell up. 
# The empty cell left over where a new customer starts is left as an NA i.e. in the latest month of the customer's life we don't know if he/she will churn next month. The important thing is that THIS month we have an indicator if he/she will churn next month.
```

```{r}
np = np %>% # Same as above, but for nextprice
  group_by(SubscriptionId) %>% 
  mutate(nextprice = lead(currprice, order_by = t))

np = np[c(1,2,3,(length(np)-1), 4:(length(np)-2), (length(np)))] # Reordering Columns so churn is next to nextchurn and currprice is close to nextprice.

head(np,100)
```

## b) Converting t to factor and displaying table(t)

```{r}
np[['t']]= factor(np[['t']])
head(np)
```


```{r}
table(np[['t']])
```

## c) - Running Logistic Regression
### Model 1

```{r}
logReg1 = glm(nextchurn ~ t+trial+nextprice+regularity+intensity, data = np, family = 'binomial')
summary(logReg1)
```

Checking the Variance Inflation Factors (VIFs)
None of them are larger than 10.

```{r}
vif(logReg1)
```


```{r}
logReg2 = glm(nextchurn ~ t+trial+nextprice+regularity, data = np, family = 'binomial')
summary(logReg2)
```

Checking the Variance Inflation Factors (VIFs)
None of them are larger than 10.

```{r}
vif(logReg2)
```

```{r}
logReg3 = glm(nextchurn ~ t+trial+nextprice+intensity, data = np, family = 'binomial')
summary(logReg3)
```

Checking the Variance Inflation Factors (VIFs)
None of them are larger than 10.

```{r}
vif(logReg3)
```

Let's look at the correlation matrix for our predictors.

```{r}
npSubset = np %>%  select(t, trial,nextchurn, nextprice, regularity, intensity)
npSubset['t'] = sapply(npSubset[['t']], as.numeric) # Converting t back to numeric for the cor() function
cor(npSubset[c(2:length(npSubset))])
```

Let's remove the NA's from the lead variables.
Note that the below has only 9,535 out of 12,170 rows from the dataset.

```{r}
cor(na.omit(npSubset[c(2:length(npSubset))]))
```

There is no correlation larger than .6, which agian doesn't suggest any significant multicollinearity.
It is worth pointing out, however, that there is significant collinearity b/w  regularity & intensity.
That's why individually, intensity and regularity appear highly significant, but when both are added to the same model only regularity is significant.

The baseline t = 1 level is the reference category for t. This means that the rest of the factors tell us how the probability of churning next month changes as time goes by FOR ALL USERS. On the other hand, trial gives us an indication how the probability that the user will churn changes after their 1-month trial ends i.e. this applies ONLY FOR USERS WHO HAD A TRIAL.

### d) Modelling with content variables

```{r}
logRegContent1 = glm(nextchurn~t+trial+nextprice+sports1+news1+crime1+life1+obits1+business1+opinion1, data = np, family = 'binomial')
summary(logRegContent1)
```

Now we include regularity in the model

```{r}
logRegContent2 = glm(nextchurn~t+trial+nextprice+regularity+sports1+news1+crime1+life1+obits1+business1+opinion1, data = np, family = 'binomial')
summary(logRegContent2)
```

After adding regularity, we see that sports1 & news1 are no longer significant.
This is probably, again, due to them being correlated with regularity.

### e) - Fitting with Location

```{r}
logRegLocation1 = glm(nextchurn~t+trial+nextprice+loc1+Loc2+Loc3+Loc4, data = np, family = 'binomial')
summary(logRegLocation1)
```

```{r}
logRegLocation2 = glm(nextchurn~t+trial+nextprice+regularity+loc1+Loc2+Loc3+Loc4, data = np, family = 'binomial')
summary(logRegLocation2)
```

Once again, after adding regularity, we observe that loc1 is no longer significant.
This probably makes sense: most users have a specific location where they read 'most regularly'.
As a result, the two variables would be highly correlated.

### f) - Effects of Source on Churn

```{r}
logRegSource1 = glm(nextchurn~t+trial+nextprice+SrcGoogle+SrcDirect+SrcElm+SrcSocial+SrcBingYahooAol+SrcNewsletter+SrcLegacy+SrcGoogleNews+SrcGoogleAd, data = np, family = 'binomial')
summary(logRegSource1)
```

Now we add the regularity variable

```{r}
logRegSource2 = glm(nextchurn~t+trial+nextprice+regularity+SrcGoogle+SrcDirect+SrcElm+SrcSocial+SrcBingYahooAol+SrcNewsletter+SrcLegacy+SrcGoogleNews+SrcGoogleAd, data = np, family = 'binomial')
summary(logRegSource2)
```

Again, the different source variables become less significant after we add regularity.
This also makes sense - they are correlated with regularity, probably because people tend to visit a given website via the same source.

### g) Effects of Device on Churn
```{r}
names(np)
```

```{r}
logRegDevice1 = glm(nextchurn~t+trial+nextprice+mobile+tablet+desktop, data = np, family = 'binomial')
summary(logRegDevice1)
```

As before, we add regularity.

```{r}
logRegDevice2 = glm(nextchurn~t+trial+nextprice+regularity+mobile+tablet+desktop, data = np, family = 'binomial')
summary(logRegDevice2)
```

Similarly to before, we see that desktop is no longer significant.

Overall, we can conclude that device, location, source, and content don't have a significant impact on churn next month.

### h) Fitting with all variables at the same time

Without regularity

```{r}
logRegAllVar1 = glm(nextchurn~t+trial+nextprice+sports1+news1+crime1+life1+obits1+business1+opinion1+mobile+tablet+desktop+loc1+Loc2+Loc3+Loc4+SrcGoogle+SrcDirect+SrcElm+SrcSocial+SrcBingYahooAol+SrcNewsletter+SrcLegacy+SrcGoogleNews+SrcGoogleAd, data = np, family = 'binomial')
summary(logRegAllVar1)
```

With regularity

```{r}
logRegAllVar2 = glm(nextchurn~t+trial+nextprice+regularity+sports1+news1+crime1+life1+obits1+business1+opinion1+mobile+tablet+desktop+loc1+Loc2+Loc3+Loc4+SrcGoogle+SrcDirect+SrcElm+SrcSocial+SrcBingYahooAol+SrcNewsletter+SrcLegacy+SrcGoogleNews+SrcGoogleAd, data = np, family = 'binomial')
summary(logRegAllVar2)
```

Our conclusions don't change in any significant way when we add all the variables at the same time with or without regularity.

Let's try to use Lasso with Cross-Validation.

```{r}
# Setting up data
set.seed(123456789)
y = na.omit(npSubset)$nextchurn
x = cbind(np[, c(2, 4)], scale(np[, c(5,7:29,31:32)])) # Scaling (aka standardizing) only the numeric variables
xx = model.matrix(nextchurn ~ ., data = x)
lassoCV = cv.glmnet(xx,y,alpha=1,nfold=10, family = 'binomial')
```

```{r}
plot(lassoCV)
```

This is our optimal lambda from the plot above.

```{r}
lassoCV$lambda.min
```

These are the coefficients for the Logistic Regression using the best lambda.

```{r}
coef(lassoCV$glmnet.fit)[, which(lassoCV$lambda == lassoCV$lambda.min)]
```

We notice that all of the variables that we previously showed as insignificant (device, content, source, location) have been shrunk to 0. In other words, we once again show that these variables don't bring any significant information.

### i) 

- No association with churn
  We are not able to conclude that the source, location, device, or content that the user consumer are predictive or associated with their probability of churn.
- Strong drivers of churn (do less of these things)
  The end of the trial and the next prince that the user will have to pay appears to be a driver of churn. Some Customer Life-Time Value analysis would be appropriate to find out if reducing the subscription cost or making the the trial longer would increase Customer Equity.
- Strong drivers of retention (do more of these things)
  Regularity appears to be a meaningful driver of retention. Engaging the users, for example via personalized emails with content appropriate for them might be a good idea to boost regularity.
- Questionable drivers of churn
  t8 & t9 - appear to be mildly significant. It might be a good idea to check if there is an increase in the harard rate at that time through some Survival Analysis. It might be that users just get bored with our content. Again, engagement and personalization might be appropriate 
- Questionable drivers of retention
  In some of our regressios, the news content variable appeared slightly significant. We suspect that news content is the main driver in user regularity (i.e. people read the news every day). Therefore, making sure that our news experience is seamless and personalized might be a driver of retention, but something, which would need more analysis and investigation.
