---
title: "PCA HW"
author: Kristiyan Dimitrov, Srividya Ganapathi, Shreyashi Ganguly, Greesham Simon,
  Joe Zhang
date: "3/8/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(psych)
library(Matrix)
```

# Problem 1

```{r}
gene = read.csv('gene.csv')
dim(gene)
```

```{r, message=FALSE, warning=FALSE}
#Checking to see if all fields are measured on comparable scales
mean <- apply(gene[,-1001],2,mean)
hist(mean)
sd <- apply(gene[,-1001],2,sd)
hist(sd)
```

All the X variables are measurements of comparable units. Also, almost all the fields have mean 0 and standard deviation 1, hence not standardizing.

## a) PCA on data

Note that we are looking at only the first 1000 columns, not including response variable sick.
Also, we're not scaling the data, because we assume all the variables are using the same units.

```{r}
fitPrComp = prcomp(gene[,1:1000], scale. = FALSE)
```

```{r}
plot(fitPrComp)
```

```{r}
summary(fitPrComp)
```

Looking at the first 2 Principal Components, it appears that PC1 & PC2 together explain 15.94% of the variance in the data.

## b) Generating a Scatter Plot of the first 2 Principal components

First we take out the scores for the 40 points in PC1 & PC2

```{r}
first2PC = fitPrComp$x[,1:2]
head(first2PC)
```

Even with just two Principal components, we can see very nice separation in the data. In other words,  first two principal components can adequately capture the distinction between sick and healthy tissue samples.

```{r}
plot(fitPrComp$x[,1:2], col = gene[,1001]+1, pch = 16)
```

Using this data for prediction with a Logictic Regression or a Tree model would probably be better.
First, Logistic Regression wouldn't even run with more variables than observations.
And moreover, using the PC results would be more computationally efficient.

# Problem 2

```{r}
X=matrix(c(1:4, 1,4,9,16), nrow=4)
X
```

## a) 
### Finding X_t * X

```{r}
first = t(X) %*% X
first
```


### Finding X * X_t

```{r}
second = X %*% t(X)
second
```

## b) Eigenvalues & Eigenvectors of X_t * X

```{r}
eigenFirst = eigen(first, symmetric = T)
eigenFirst
```

## c) Eigenvalues & Eigenvectors of X * X_t

```{r}
eigenSecond = eigen(second, symmetric = F)
eigenSecond
```

# d) Commenting on the above results
We note that the first two eigen values of X * X_t are the same as those of X_t * X.
X * X_t has two additional eigen values (owing to itâ€™s 4x4 structure), but they are approximately 0.

We also see that the first eigen vector for X_t * X is much bigger than the second.
This suggests that most of the variance will be explained with the first PC.

```{r}
eigenFirst$values
eigenSecond$values
```

# Problem 3

```{r}
X = matrix(c(1,3,5,0,1, 2,4,4,2,3, 3,5,3,4,5), nrow=5)
X
```

## a) 
### Finding X_t * X

```{r}
first = t(X) %*% X
first
```


### Finding X * X_t

```{r}
second = X %*% t(X)
second
```

## b) Eigenvalues & Eigenvectors 
### of X_t * X

```{r}
eigenFirst = eigen(first, symmetric = T)
eigenFirst
```

### of X * X_t

```{r}
eigenSecond = eigen(second, symmetric = F)
eigenSecond
```

## c) Confirming X_t * X == Gamma * Lambda * Gamma_Transpose

Creating LAMBDA (Eigenvalues diagonal matrix)

```{r}
LAMBDA = Diagonal(3, eigenFirst$values)
LAMBDA
```

Creating GAMMA (EigenVector Matrix)

```{r}
GAMMA = eigenFirst$vectors
GAMMA
```

Checking equation

```{r}
round(GAMMA %*% LAMBDA %*% t(GAMMA),5) == first
```

Yes! The equation is confirmed.

## d) SVD

```{r}
SVD = svd(X)
```

We see that the Singular Value Decomposition gives us the same results as b) i.e. the eigen vectors of X_t * X.
Moreover, as expected, they differ only up to a sign difference.

```{r}
SVD$v
```

```{r}
eigenFirst$vectors
```

## e) Confirming SVD Equation

We perform the calculations below and observe that they are equal to X!

```{r}
round(SVD$u %*% Diagonal(3, SVD$d) %*% t(SVD$v), 2)
```

```{r}
round(SVD$u %*% Diagonal(3, SVD$d) %*% t(SVD$v), 2) == X
```

## f) 

Setting the smallest singular value to 0. Note that the 3rd Singular value is already 0, because the rank of our matrix is 2 i.e. only 2 of the columns are linearly independent (more specifically two times the second minus the third gives us the first)

```{r}
dModified = diag(c(SVD$d[1],0,0))
dModified
```

```{r}
Xhat = SVD$u %*% dModified %*% t(SVD$v)
Xhat
```

Why is this a one-dimensional representation of X?
Well, we still see that two times the second column of Xhat minus the 3rd column gives us the 1st column
```{r}
Xhat[,2]*2 - Xhat[,3]
```

```{r}
Xhat[,1]
```

Furthermore, we see that the 2nd column is an exact scalar multiple of the 1st column.
In fact, all three columns are scalar multiples of each other, therefore the rank of Xhat is 1 and the span is a 1-dimensional line.

```{r}
Xhat[,3] / Xhat[,2]
```

```{r}
Xhat[,2] / Xhat[,1]
```

Furthermore, we can calculate the values of Xhat on this one dimensional space in two ways.
In other words, these would be the principal component scores (the matrix Y as defined on page 24 of the notes)
Calculating the one-dimensional representation of X in two ways:

First way: X * V[1] i.e. projecting X on the first principal component

```{r}
Xhat1 =  X %*% SVD$v[,1]
Xhat1
```

Second Way: U * D_modified, where D_modified is the diagonal matrix which contains only the first singular value.

```{r}
Xhat2 = SVD$u %*% dModified
Xhat2
```

We confirm we get the same result!

## g) Sum of Squared Values of Xhat

The sum of the squared values of Xhat is precisely the square of the singular value.

```{r}
sum(Xhat^2)
```

```{r}
sum(dModified^2)
```

It is also precisely the square sum of the principal component scores!

```{r}
sum(Xhat1^2)
```

In other words the variance of the principal component scores is precisely equal to the square value of the singular value and equal to the eigen vector along that principal component. 
This makes sense, the squared singular value represents the variance along a given principal component.
So when we project our data onto that principal component, we would expect precisely that amount of variance.

## h) X - Xhat, square the values & add them up

We observe that the 'sum of squared errors' X-Xhat is precisely the variance represented by the squared value of the 2nd singular value (the one we set to 0 in part (f))

```{r}
sum((X-Xhat)^2)
```

```{r}
SVD$d[2]^2
```

This makes sense, that's precisely the amount of variance we lost by ignoring that singular value and projecting into a lower-dimensional subspace.

## i) How much energy is preserved with the first singular value?

We assume this means what proportion of the squared singular values i.e. what percent of the variance is explained,
because the squared singular values are the eigenvectors of X_t * X.
Below we compute in two different, but equivalent ways.

```{r}
(SVD$d[1])^2 / sum(SVD$d^2)
```

```{r}
sum(Xhat1^2) / (sum(Xhat1^2) + sum((X-Xhat)^2))
```

It appears that 90.86% of the 'energy' i.e. variance is preserved with the first singular value.

# Problem 4

## a) Commenting on the Correlation Matrix R

```{r}
R = matrix(c(
1, .4919, .2636, .4653, -.2277, .0652,
.4919, 1, .3127, .3506, -.1917, .2045,
.2635, .3127, 1, .4108, .0647, .2493,
.4653, .3506, .4108, 1, -.2249, .2293,
-.2277, -.1917, .0647, -.2249, 1, -.2144,
.0652, .2045, .2493, .2293, -.2144, 1),
byrow=T, ncol=6)

fishNames = c('Bluegill','Black crappie', 'Smallmouth bass',
'Largemouth bass', 'Walleye', 'Northern pike')
rownames(R) = fishNames
colnames(R) = fishNames
R
```

Let's look at the first 4 rows & columns, the centrarchids family

```{r}
R[1:4,1:4]
```

We observe that the correlations within the centrarchids family is much greater than with the other types of fish (walleye and Northern pike).
In fact, Walleye is negatively correlated with most of the centrarchids family.

## b) PCA on the centrarchid family

```{r}
eigen(R[1:4,1:4])
```

PC1 (corresponding to the first eigenvector) has more or less equal scores for all variables; In other words, it is a sort of average and simply suggests how much fish fishermen catch.

PC2 separates feshermen into two groups: those that catch more Bluegill & Black crappie vs. those that catch more Small & Large mouth bass.

## c) PCA on entire correlation matrix R

```{r}
eigenFit = eigen(R)
eigenFit
```

Creating a scree plot

```{r}
plot(1:6, eigenFit$values, type = 'b')
```

## d)

```{r}
sum(eigenFit$values[1:2]) / sum(eigenFit$values)
```

We observe that 57.11% of the variation is accounted for by the first two PCs in (c)

## e) Interpreting first loading vector of part (c)

```{r}
rownames(eigenFit$vectors) <- fishNames
colnames(eigenFit$vectors) <- fishNames
eigenFit$vectors
```

The first loading vector clearly separates all the fishermen into those that catch Walleye vs. those that catch all the other types of fish.
The problem states that Walleye is the most popular fish to eat.
Perhaps this means there are two kinds of fishermen - those who fish for the fun of it and don't care what they catch and those who are specifically fishing to catch the tastiest fish!








