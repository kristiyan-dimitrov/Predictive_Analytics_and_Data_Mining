---
title: "Homework 4"
author: "Kristiyan Dimitrov"
date: "12/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DescTools)
library(caret)
library(DMwR)
# install.packages("mice")
library(mice)
# install.packages("missForest")
library(missForest)
```

- Impute the missing values in the training data based on the observed values in the training data
- AFTER THAT, train linear model on top of the imputed training data


## Exercise 1

```{r}
# Import Data
filepath = "redwine.txt"
redwine <- read.csv(file = filepath, sep = "\t", stringsAsFactors = FALSE)
redwine <- prodNA(redwine, noNA = .1)
str(redwine)
```

### a)
I find histograms more intuitive and easier to work with

```{r}
# Plotting all 12 histograms at the same time
redwine %>% gather() %>% ggplot( aes(value)) + 
                          geom_histogram(bins = 30) + 
                          facet_wrap(~key, scales = 'free_x')
```


```{r}
# Plot individual histogram as sanity check
hist(redwine$DE ) #, breaks = seq(.99,1.005,.0005))
```

### Part b) - Boxplots
I decided to plot variables with similar scales/ranges on the same boxplot

```{r}
scale1 <- c("CA","VA","SU")
boxplot(redwine[scale1])
```
The above box plot shows multiple outliers on the top end, particularly for volatile.acidity & sulphates.

```{r}
scale2 <- c("AL", "QA")
boxplot(redwine[scale2])
```

The above boxplot shows just a few outliers

```{r}
boxplot(redwine["CH"])
```

The above boxplot shows a few outliers on the bottom part, and many on the top part.

```{r}
boxplot(redwine["DE"])
```

Multiple outliers on either side of the range.

```{r}
boxplot(redwine["FA"])
boxplot(redwine["FS"])
```

Multiple outliers for both varibles on the top end of the range

```{r}
boxplot(redwine$SU)
```
Many outliers on the top part of the range

```{r}
boxplot(redwine$PH)
```
Multiple outliers on either side

```{r}
boxplot(redwine$SD)
```

Many outliers in top range, with two particularly egregious outliers

### Part c)

```{r}
?Skew
for (i in colnames(redwine)){
  print(i)
  print(paste("Skewness:", round(Skew(redwine[,i], na.rm = TRUE),4), " Kurtosis:",round(Kurt(redwine[,i], na.rm = TRUE),4)))
  
  if (round(Skew(redwine[,i], na.rm = TRUE),4) >= 0){
    print(paste(i, "is right skewed"))
  } else {
    print(paste(i, "is left skewed"))
  }
  
  
  if (round(Kurt(redwine[,i], na.rm = TRUE),4) < -.5){
    print(paste(i, "is platykurtic"))
  } 
  
  if(round(Kurt(redwine[,i], na.rm = TRUE),4) > .5){
    print(paste(i, "is leptokurtic"))
  }
  
   if(round(Kurt(redwine[,i], na.rm = TRUE),4) < .5 && round(Kurt(redwine[,i], na.rm = TRUE),4) > -.5){
    print(paste(i, "is mesokurtic"))
  }
  
  print("------------------------")
  
}


```

Based on the low value of skewness for quality and density, we can say that these variables are fairly symmetric (not significantly skewed)

### Part d)

Note that for the normal distribution the Skewness is 0 and the Kurtosis is 3

```{r}

for (i in colnames(redwine)){
  qqnorm(redwine[,i], main = paste("Normal Q-Q Plot for",i))
  qqline(redwine[,i])
}
```

Observations:  
- Multiple graphs show outliers at the top range of the variable. Similar observations were made from the boxplots for those variables (e.g. total.sulfur.dioxide and chlorides)  
- Furthermore, Skewness & Kurtosis observatios are supported by the Q-Q plots. For example, we see that the chlorides variable has many many values above the standard normal Quantiles line in the upper range. This is also the reason it has high skewness and very high Kurtosis (i.e. a very fat right tail compared to the standard normal distribution)  
- We also observe that the more the Q-Q plot looks like an exponential function, the higher the kurtosis and the more the distribution is leptokurtic. The only variable, which has a different looking Q-Q plot, is the CA variable, whose Q-Q plot looks logarithmic. This is also supported by the fact that it is platikurtic i.e. the shape of the Q-Q plot and the variable distribution being platy/leptu-kurtic are connected.

## Problem 2

### Part a)
```{r}
colSums(is.na(redwine))
```
We see that Total Sulfur Dioxide (SD) has 17 missing values and Residual Sugar (RS) has 22 missing values


```{r}
sum(rowSums(is.na(redwine)))
```

We confirm the above result by checking that there are a total of 39 samples with missing values in the data

### Part b) - Imputation via Random Sampling from observed data

```{r}
# Creating a couple of vectors to capture all the averages produces from the different imputation methods
avgTrainMSEs <- c()
avgTestMSEs <- c()
```


```{r}
set.seed(157)
folds = createFolds(1:nrow(redwine), k = 5)
folds <- unname(folds)
```

```{r}
MSE.training <- c() # Initialize vectors to store MSE on training set
MSE.test <- c() # Initialize vectors to store MSE on test set

for (j in seq(1,5)){ # Take each fold of data one at a time
  
  testFold <- redwine[unlist(folds[j]),] # Take indices from the j-th fold and retrieve the redwine data for them
  trainingFolds <- redwine[unlist(folds[-j]),] # Take the indices in all the folds except the j-th one
  # and retrieve the corresponding redwine data as training set
  
  for(i in seq(1,ncol(testFold))){ # Take one column at a time from the testFold
    missing = is.na(testFold[,i]) # Find all the locations where there is a missing value in that column
    n.missing = sum(missing) # Find the number of missing values
    observed_values = na.omit(trainingFolds[,i]) # Take all the non-NA values from the i-th column of the trainingFolds
    
    trainMissing <- is.na(trainingFolds[,i]) # Find all the locations in the training data, where there is a missing value
    n.trainMissing <- sum(trainMissing) # Find the number of missing values in teh training dataset
    
    trainingFolds[,i][trainMissing] <- sample(observed_values, n.trainMissing, replace = TRUE)# Impute the missing values in the training dataset
    
    testFold[,i][missing] <- sample(observed_values, n.missing, replace = TRUE) # Replace all the missing values 
    # with random samples from the training Folds
  }

  lmfit <- lm(QA ~ ., data = trainingFolds) # Fit linear regression on trainingFolds (which is now imputed and should have no missing values)
  MSE.training <- c(MSE.training, mean(lmfit$residuals^2)) # Compute MSE for trainingFolds and store it

  testFold.predicted <- predict(lmfit, newdata = testFold) # Make predictions on testFold
  MSE.test <- c(MSE.test, mean((testFold[,1] - testFold.predicted)^2)) # Compute MSE for testFold and store it
}

avgTrainMSEs <- c(avgTrainMSEs, mean(MSE.training))
avgTestMSEs <- c(avgTestMSEs, mean(MSE.test))

print(MSE.training)
print(MSE.test)
print(paste("The average MSE for the trainingFolds is:", mean(MSE.training)))
print(paste("The average MSE for the testFold is:", mean(MSE.test)))
```

### Part c) - Imputation via Most Common Value (Mode)

```{r}
folds = createFolds(1:nrow(redwine), k = 5)
folds <- unname(folds)
```

```{r}
MSE.training <- c() # Initialize vectors to store MSE on training set
MSE.test <- c() # Initialize vectors to store MSE on test set

for (j in seq(1,5)){ # Take each fold of data one at a time
  
  testFold <- redwine[unlist(folds[j]),] # Take indices from the j-th fold and retrieve the redwine data for them
  trainingFolds <- redwine[unlist(folds[-j]),] # Take the indices in all the folds except the j-th one
  # and retrieve the corresponding redwine data as training set
  
  for(i in seq(1,ncol(testFold))){ # Take one column at a time from the testFold
    missing = is.na(testFold[,i]) # Find all the locations where there is a missing value in that column
    n.missing = sum(missing) # Find the number of missing values
    observed_values = na.omit(trainingFolds[,i]) # Take all the non-NA values from the i-th column of the trainingFolds
    
    trainMissing <- is.na(trainingFolds[,i]) # Find all the locations in the training data, where there is a missing value
    n.trainMissing <- sum(trainMissing) # Find the number of missing values in the training dataset
    
    mode = as.numeric(names(sort(table(observed_values), decreasing = TRUE))[1]) # Finding the mode of the trainingFolds data
    
    trainingFolds[,i][trainMissing] <- rep(mode, n.trainMissing) # Impute the missing values in the trainingFolds
    testFold[,i][missing] <- rep(mode, n.missing) # replacing the missing values with the mode
  }

  lmfit <- lm(QA ~ ., data = trainingFolds) # Fit linear regression on trainingFolds (which is now imputed and should have no missing values)
  MSE.training <- c(MSE.training, mean(lmfit$residuals^2)) # Compute MSE for trainingFolds and store it

  testFold.predicted <- predict(lmfit, newdata = testFold) # Make predictions on testFold
  MSE.test <- c(MSE.test, mean((testFold[,1] - testFold.predicted)^2)) # Compute MSE for testFold and store it
}

avgTrainMSEs <- c(avgTrainMSEs, mean(MSE.training))
avgTestMSEs <- c(avgTestMSEs, mean(MSE.test))

print(MSE.training)
print(MSE.test)
print(paste("The average MSE for the trainingFolds is:", mean(MSE.training)))
print(paste("The average MSE for the testFold is:", mean(MSE.test)))
```

### Part d) - Imputation via Average Value (Mean)

```{r}
folds = createFolds(1:nrow(redwine), k = 5)
folds <- unname(folds)
```

```{r}
MSE.training <- c() # Initialize vectors to store MSE on training set
MSE.test <- c() # Initialize vectors to store MSE on test set

for (j in seq(1,5)){ # Take each fold of data one at a time
  
  testFold <- redwine[unlist(folds[j]),] # Take indices from the j-th fold and retrieve the redwine data for them
  trainingFolds <- redwine[unlist(folds[-j]),] # Take the indices in all the folds except the j-th one
  # and retrieve the corresponding redwine data as training set
  
  for(i in seq(1,ncol(testFold))){ # Take one column at a time from the testFold
    missing = is.na(testFold[,i]) # Find all the locations where there is a missing value in that column
    n.missing = sum(missing) # Find the number of missing values
    observed_values = na.omit(trainingFolds[,i]) # Take all the non-NA values from the i-th column of the trainingFolds
    
    avg = mean(observed_values) # Finding the average value of the trainingFolds data
    trainingFolds[,i][trainMissing] <- rep(avg, n.trainMissing) # Impute the missing values in the trainingFolds
    testFold[,i][missing] <- rep(avg, n.missing) # replacing the missing values with the mean in the testFold

  }

  lmfit <- lm(QA ~ ., data = trainingFolds) # Fit linear regression on trainingFolds (excluding incomplete observations!)
  MSE.training <- c(MSE.training, mean(lmfit$residuals^2)) # Compute MSE for trainingFolds and store it

  testFold.predicted <- predict(lmfit, newdata = testFold) # Make predictions on testFold
  MSE.test <- c(MSE.test, mean((testFold[,1] - testFold.predicted)^2)) # Compute MSE for testFold and store it
}

avgTrainMSEs <- c(avgTrainMSEs, mean(MSE.training))
avgTestMSEs <- c(avgTestMSEs, mean(MSE.test))

print(MSE.training)
print(MSE.test)
print(paste("The average MSE for the trainingFolds is:", mean(MSE.training)))
print(paste("The average MSE for the testFold is:", mean(MSE.test)))
```

### Part e) - Imputation via k-Nearest Neighbours (k-NN)

```{r}
set.seed(157)
folds = createFolds(1:nrow(redwine), k = 5)
folds <- unname(folds)
```

```{r}
MSE.training <- c() # Initialize vectors to store MSE on training set
MSE.test <- c() # Initialize vectors to store MSE on test set

for (j in seq(1,5)){ # Take each fold of data one at a time
  
  testFold <- redwine[unlist(folds[j]),] # Take indices from the j-th fold and retrieve the redwine data for them
  trainingFolds <- redwine[unlist(folds[-j]),] # Take the indices in all the folds except the j-th one
  # and retrieve the corresponding redwine data as training set

  # The knnImputation function provides very convenient argument for training data
  testFold <- knnImputation(testFold, k = 5, distData = na.omit(trainingFolds)) 
  trainingFolds <- knnImputation(trainingFolds, k = 5)
  
  lmfit <- lm(QA ~ ., data = trainingFolds) # Fit linear regression on trainingFolds (excluding incomplete observations!)
  MSE.training <- c(MSE.training, mean(lmfit$residuals^2)) # Compute MSE for trainingFolds and store it

  testFold.predicted <- predict(lmfit, newdata = testFold) # Make predictions on testFold
  MSE.test <- c(MSE.test, mean((testFold[,1] - testFold.predicted)^2)) # Compute MSE for testFold and store it
}

avgTrainMSEs <- c(avgTrainMSEs, mean(MSE.training))
avgTestMSEs <- c(avgTestMSEs, mean(MSE.test))

print(MSE.training)
print(MSE.test)
print(paste("The average MSE for the trainingFolds is:", mean(MSE.training)))
print(paste("The average MSE for the testFold is:", mean(MSE.test)))
```

### Parf f) Multivariate Imputation via Chained Equations (MICE)

```{r}
set.seed(157)
folds = createFolds(1:nrow(redwine), k = 5)
folds <- unname(folds)
```

```{r}
MSE.training <- c() # Initialize vectors to store MSE on training set
MSE.test <- c() # Initialize vectors to store MSE on test set

for (j in seq(1,5)){ # Take each fold of data one at a time
  
  testFold <- redwine[unlist(folds[j]),] # Take indices from the j-th fold and retrieve the redwine data for them
  trainingFolds <- redwine[unlist(folds[-j]),] # Take the indices in all the folds except the j-th one
  # and retrieve the corresponding redwine data as training set
  
  testFold.mice <- mice(testFold, printFlag = F) # Get MICE Imputations
  
  # Finding all the locations with missing values for $RS & $SD columns
  missingQA = is.na(testFold$QA)
  missingFA = is.na(testFold$FA)
  missingVA = is.na(testFold$VA)
  missingCA = is.na(testFold$CA)
  missingCH = is.na(testFold$CH)
  missingFS = is.na(testFold$FS)
  missingDE = is.na(testFold$DE)
  missingPH = is.na(testFold$PH)
  missingSU = is.na(testFold$SU)
  missingAL = is.na(testFold$AL)
  missingRS = is.na(testFold$RS)
  missingSD = is.na(testFold$SD)
  
  # Setting those missing values to the values imputed via the 5th iteration of MICE
  testFold[missingRS,]$RS <- testFold.mice$imp$RS[,5]
  testFold[missingSD,]$SD <- testFold.mice$imp$SD[,5]
  testFold[missingFA,]$FA <- testFold.mice$imp$FA[,5]
  testFold[missingVA,]$VA <- testFold.mice$imp$VA[,5]
  testFold[missingCA,]$CA <- testFold.mice$imp$CA[,5]
  testFold[missingCH,]$CH <- testFold.mice$imp$CH[,5]
  testFold[missingFS,]$FS <- testFold.mice$imp$FS[,5]
  testFold[missingDE,]$DE <- testFold.mice$imp$DE[,5]
  testFold[missingPH,]$PH <- testFold.mice$imp$PH[,5]
  testFold[missingSU,]$SU <- testFold.mice$imp$SU[,5]
  testFold[missingAL,]$AL <- testFold.mice$imp$AL[,5]
  testFold[missingRS,]$RS <- testFold.mice$imp$RS[,5]
  testFold[missingSD,]$SD <- testFold.mice$imp$SD[,5]

  lmfit <- lm(QA ~ ., data = na.omit(trainingFolds)) # Fit linear regression on trainingFolds (excluding incomplete observations!)
  MSE.training <- c(MSE.training, mean(lmfit$residuals^2)) # Compute MSE for trainingFolds and store it

  testFold.predicted <- predict(lmfit, newdata = testFold) # Make predictions on testFold
  MSE.test <- c(MSE.test, mean((testFold[,1] - testFold.predicted)^2)) # Compute MSE for testFold and store it
}

avgTrainMSEs <- c(avgTrainMSEs, mean(MSE.training))
avgTestMSEs <- c(avgTestMSEs, mean(MSE.test))

print(MSE.training)
print(MSE.test)
print(paste("The average MSE for the trainingFolds is:", mean(MSE.training)))
print(paste("The average MSE for the testFold is:", mean(MSE.test)))
```

### Part g) Simply Removing the NA values

```{r}
set.seed(157)
folds = createFolds(1:nrow(redwine), k = 5)
folds <- unname(folds)
```

```{r}
MSE.training <- c() # Initialize vectors to store MSE on training set
MSE.test <- c() # Initialize vectors to store MSE on test set

for (j in seq(1,5)){ # Take each fold of data one at a time
  
  testFold <- redwine[unlist(folds[j]),] # Take indices from the j-th fold and retrieve the redwine data for them
  trainingFolds <- redwine[unlist(folds[-j]),] # Take the indices in all the folds except the j-th one
  # and retrieve the corresponding redwine data as training set
  
  testFold <- na.omit(testFold) # Removing observations with missing values
  
  lmfit <- lm(QA ~ ., data = na.omit(trainingFolds)) # Fit linear regression on trainingFolds (excluding incomplete observations!)
  MSE.training <- c(MSE.training, mean(lmfit$residuals^2)) # Compute MSE for trainingFolds and store it

  testFold.predicted <- predict(lmfit, newdata = testFold) # Make predictions on testFold
  MSE.test <- c(MSE.test, mean((testFold[,1] - testFold.predicted)^2)) # Compute MSE for testFold and store it
}

avgTrainMSEs <- c(avgTrainMSEs, mean(MSE.training))
avgTestMSEs <- c(avgTestMSEs, mean(MSE.test))

print(MSE.training)
print(MSE.test)
print(paste("The average MSE for the trainingFolds is:", mean(MSE.training)))
print(paste("The average MSE for the testFold is:", mean(MSE.test)))
```

### Part h
```{r}
MSE.dt <- data.frame(rbind(avgTrainMSEs, avgTestMSEs))
colnames(MSE.dt) <- c("Random Sampling", "Mode", "Mean", "kNN", "MICE", "Omitting NAs")
print(MSE.dt)
```

Observations:
- kNN performed better, followed by Random Sampling & MICE  
- Mode, Mean, and Omitting NAs performed more poorly  
- My hypothesis is that different imputation methods probably have different levels of effectiveness based on the shape of the data they are applied to  
- In our case, we were imputing the RS & SD data.  
- In problem 1, we found out they are both leptykurtic and right skewed.  
- Much more experimentation would be needed to try and detect any relationship between skewness, kurtosis, and imputation effectiveness  
- Of course, it probably also just matters on the specific dataset.  

- It is important to note, that the differences between the MSEs of the different models are very small due to the low number of missing values.  
- In order to investigate, I copied the entire .Rmd file and artificially created 10% missing values  

```{r}
# # install.packages("missForest")
# library(missForest)
# # Import Data
# filepath = "redwine.txt"
# redwine <- read.csv(file = filepath, sep = "\t", stringsAsFactors = FALSE)
# redwine <- prodNA(redwine, noNA = .1)
# str(redwine)
```

The result was surprising:
 
                Random Sampling    Mode       Mean          kNN          MICE    Omitting NAs
    avgTrainMSEs	0.4482886	0.4487023	0.4494927	0.4482886	0.4482886	0.4482886
    avgTestMSEs	    0.5143323	0.4728982	0.4309251	0.4014888	0.4652415	0.4853339

Random Sampling was the best and Mode & Mean did better than in our original scenario.  
Interestingly, kNN performed the worst!  
Somewhat disappointing that Random Sampling & Omitting performed the best on test data.  


References:
  P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. 
  Modeling wine preferences by data mining from physicochemical properties.
  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.
 